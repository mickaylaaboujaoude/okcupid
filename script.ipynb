{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abab5896",
   "metadata": {},
   "source": [
    "### Import Python packages and read csv in to Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd72c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('../original_data/okcupid/okcupid_profiles.csv')\n",
    "#df = pd.read_csv('../original_data/okcupid/okcupid_profiles.csv', index_col = False)\n",
    "\n",
    "# Do not do this one (index_col =0). It removes age, etc.\n",
    "# I don't remember the details. Just don't use it in this context\n",
    "#df = pd.read_csv('../original_data/okcupid/okcupid_profiles.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281998c5",
   "metadata": {},
   "source": [
    "### On the index / wanting an id column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106a5960",
   "metadata": {},
   "source": [
    "###### Initially (days ago):\n",
    "I don't think I need to reset the index because there is not an index column in the csv, so pd will add one upon reading.\n",
    "\n",
    "###### 10/1/21 2:45pm ish, 4:06pm \n",
    "I left off messing around with index of the df. I noticed the df doesn't have an \"id\" column, which might be nice for groupby so i was trying to make one\n",
    "\n",
    "I think the index exists regardless of whether there is a literal column in the dataframe for the index?\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/dev/reference/indexing.html <br>\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html <br>\n",
    "https://towardsdatascience.com/pandas-index-explained-b131beaf6f7b <br>\n",
    "\n",
    "###### 10/1/21 4:06pm \n",
    "Anyway, I wanted an id column whose values are 0 - # of dataframe rows - 1 and increments by 1. I thought maybe looking up index might help me get it. \n",
    "\n",
    "\n",
    "But now I'm just going to make an id column as follows:\n",
    "\n",
    "###### 10/6/21 8:53pm\n",
    "I've just confused myself about id and count... I think I'm actually making a big deal out of nothing... \n",
    "i was like wait shouldn't id = 1 for every row...idk\n",
    "i feel like if i talked to someone i'd get out of my head and clear this right up. it would be silly but helpful\n",
    "\n",
    "###### 10/7/21 12:06pm\n",
    "id was a column I created just cus I wanted to have it so I could use it for groupby (especially counting)but I could have probably used any column in place of id for groupby I'm pretty sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eaa4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_column = pd.Series([x for x in range (0, len(df['age']), 1)])\n",
    "#i could use a lamda function or i could do list comprehension \n",
    "\n",
    "print(id_column.min())\n",
    "print(id_column.max())\n",
    "\n",
    "df['id'] = id_column\n",
    "#print(df.info())\n",
    "print(df['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9d141a",
   "metadata": {},
   "source": [
    "### General dataframe info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7571cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of data{}\".format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8237c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "print(len(df.drugs))\n",
    "#print(df.columns)\n",
    "#print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc3f11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head()) # I put this in a separate cell because sometimes I like to run commands without this one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a053d5",
   "metadata": {},
   "source": [
    "### Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5aa744",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df.duplicated()\n",
    "print(duplicates.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650a44f0",
   "metadata": {},
   "source": [
    "There are no duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e0bc2a",
   "metadata": {},
   "source": [
    "### Check for missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb800c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can see from df.info() that some variables have missing values, but I prefer to also use these commands to check\n",
    "# This tells us if there are null values somewhere in the df\n",
    "print(df.isnull().values.any())\n",
    "\n",
    "# This prints out the sum of null values by column \n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01c88b9",
   "metadata": {},
   "source": [
    "There are misssing values in some columns, and I will address it later/as it comes up for analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f00cb92",
   "metadata": {},
   "source": [
    "### Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a3dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.sex.unique()) # Checking what the possible values of sex variable are\n",
    "print(pd.crosstab(index=df['sex'], columns='count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac645ac",
   "metadata": {},
   "source": [
    "In Stata you could do \"tab variable col\" (I think it was col as opposed to row) to get\n",
    "a table that had the frequencies in one column and relative frequencies (percentages) in the second column. I am trying to figure out how to do that in Pandas. I don't want just the percentages; I want both in one table. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116d261c",
   "metadata": {},
   "source": [
    "### Age\n",
    "Age does not have any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7475001",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['age'].nunique())\n",
    "print(df['age'].dtypes)\n",
    "print(df['age'].min())\n",
    "print(df['age'].max())\n",
    "# the max age is 110... I doubt an OKCupid user was actually 110. Someone was probably messing around "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac2c9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.age.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41502420",
   "metadata": {},
   "source": [
    "I had a hunch that these two would produce essentially the same output (a one way frequency table)\n",
    "\n",
    "```\n",
    "print(pd.crosstab(index=df['age'], columns='count'))\n",
    "print(df.groupby('age').id.count().reset_index().rename(columns={\"id\": \"counts\"}))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca6350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.crosstab(index=df['age'], columns='count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fc7e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby('age').id.count().reset_index().rename(columns={\"id\": \"counts\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067198a9",
   "metadata": {},
   "source": [
    "#### Check to see if the values for age go from 18 - 110 continuously, or if there is a integer in there that is missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49715d53",
   "metadata": {},
   "source": [
    "I want to make a histogram with age. <br>\n",
    "The first thing I want to do is check to see if the values for age go from 18 - 110 continuously, or if there is a integer in there that is missing<br>\n",
    "I don't know if this part is necessary, but I want to do it anyways. I'm curious.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac71ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df['age'].unique())\n",
    "\n",
    "# Starting dataframe\n",
    "age_freq = df.groupby('age').id.count().reset_index().rename(columns={\"id\": \"counts\"})\n",
    "print(type(age_freq))\n",
    "print(age_freq.columns)\n",
    "\n",
    "# Pull the list of ages only \n",
    "ages_uniqarray = age_freq['age']\n",
    "print(type(ages_uniqarray))\n",
    "# Change the type to a numpy array\n",
    "# because I want to do the comparisons with numpy\n",
    "ages_uniqarray = np.array(ages_uniqarray)\n",
    "print(type(ages_uniqarray))\n",
    "\n",
    "# Creating the comparison array\n",
    "compare_array = np.array([x for x in range(18, 111, 1)])\n",
    "print(compare_array.min())\n",
    "print(compare_array.max())\n",
    "\n",
    "\n",
    "# Check to see if ages contained in ages_uniqarray \n",
    "# are identical to the ages in compare_array\n",
    "# ages_uniqarray compare_array\n",
    "\n",
    "# numpy.array_equal \n",
    "# will return True if two arrays have the same shape and elements, \n",
    "# False otherwise\n",
    "\n",
    "# first just curious to learn more about the two arrays\n",
    "print(\"ages_uniqarray\")\n",
    "print(ages_uniqarray.ndim)\n",
    "print(ages_uniqarray.size)\n",
    "print(ages_uniqarray.shape)\n",
    "\n",
    "print(\"compare_array\")\n",
    "print(compare_array.ndim)\n",
    "print(compare_array.size)\n",
    "print(compare_array.shape)\n",
    "\n",
    "# ndarray.ndim will tell you the number of axes, or dimensions, of the array.\n",
    "\n",
    "# ndarray.size will tell you the total number of elements of the array. \n",
    "# This is the product of the elements of the array’s shape.\n",
    "\n",
    "# ndarray.shape will display a tuple of integers that indicate the number of elements stored \n",
    "# along each dimension of the array. \n",
    "# If, for example, you have a 2-D array with 2 rows and 3 columns, the shape of your array is (2, 3).\n",
    "\n",
    "print(110 - 17)\n",
    "\n",
    "np.array_equal(ages_uniqarray, compare_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0ac449",
   "metadata": {},
   "source": [
    "Actually using .ndim, .size, .shape, before even doing numpy.array_equal, tells me that it does not appear that there is an age for every int between 18 - 110 inclusive.\n",
    "\n",
    "I don't know if this was necessary. I'm going to make hisograms for age now anyways. But I was curious. And it was a fun learning exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe2940f",
   "metadata": {},
   "source": [
    "#### Binning ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a5a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins\n",
    "# I'm just making this for the heck of it\n",
    "bins = [x for x in range(18, 111, 4)]\n",
    "print(bins)\n",
    "\n",
    "df['binned_age'] = pd.cut(df['age'], bins)\n",
    "\n",
    "print(df['binned_age'].dtypes)\n",
    "#print(df[['binned_age', 'age']].head())\n",
    "#print(len(df['age']))\n",
    "print(pd.crosstab(index=df['binned_age'], columns='count'))\n",
    "#print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4a470f",
   "metadata": {},
   "source": [
    "#### Histogram for age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfda53eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_array = df['age'].values\n",
    "print(type(age_array))\n",
    "\n",
    "print(age_array.min())\n",
    "print(age_array.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cb9a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_hist1 = np.histogram(age_array, range=(18, 111), bins=14)\n",
    "print(age_hist1)\n",
    "# This numpy histogram isn't doing much for me. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca236fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ages_hist2 = plt.hist(age_array, range=(0, 111), bins=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795f9e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "ages_hist2 = plt.hist(age_array, range=(18, 111), bins=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbcb2d5",
   "metadata": {},
   "source": [
    "#### Discussion of age\n",
    "Age does not have any missing values.\n",
    "\n",
    "75th percentile = 37\n",
    "\n",
    "Outliers: 109, 110\n",
    "There is one person for whom age = 109 and one person for whom age = 110.\n",
    "It is unlikely that there was an actual OKCupid user of the age 109 or 110. \n",
    "\n",
    "\n",
    "109 and 110 are not showing up on any variations of histogram due to scaling. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a0e9e",
   "metadata": {},
   "source": [
    "### Income "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50710d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the income variable here\n",
    "print(df.income.isnull().sum())\n",
    "# While this tells us there are no missing in income, -1 indicates the person chose not to provide income\n",
    "\n",
    "print(df.income.nunique())\n",
    "print(df.income.unique())\n",
    "# 0ne-way freq table of income\n",
    "print(pd.crosstab(index=df.income, columns='count'))\n",
    "\n",
    "# Percent -1 AKA missing\n",
    "print((48442/59946)*100)\n",
    "\n",
    "# Count of rows where income != -1\n",
    "print(59946 - 48442)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0d23c6",
   "metadata": {},
   "source": [
    "While print(df.income.isnull().sum()) tells us there are no missing income values, -1 indicates the person chose not to provide an income value ([via okcupid_codebook_revised.txt](https://github.com/rudeboybert/JSE_OkCupid/blob/master/okcupid_codebook_revised.txt)).\n",
    "We can consider -1 values as missing values. \n",
    "From the one-way frequency table of income, we can see that 80% of respondents did not provide an income value. So the income variable is missing ~80% of responses. This makes the income variable less useful. \n",
    "\n",
    "There are situations where I could justify replacing missing numerical data with an average or some other value, but replacing 80% of income data with fill-ins for missing values will affect the integrity of the data. I don't think anything usefull would be gained from such an endeavor.\n",
    "\n",
    "I can do income-related analyses with the ~20% of respondents who did provide an income value, but I cannot use these analyses to make statements about the entire 59946 respondents as a whole. I will do analyses with the ~20% of respondents who did provide an income value for the purpose of demonstrating my quantitative data analysis skills. (e.g. a histogram of income responses)\n",
    "\n",
    "Note that the income responses are already discrete as opposed to continuous. \n",
    "\n",
    "For the record, as an OKCupid user (in 2014 but not in 2015), I have no recollection of OKCupid ever asking for income data, but perhaps I just forgot or overlooked that question.\n",
    "\n",
    "Count of rows where income != -1 is 11504"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546567f7",
   "metadata": {},
   "source": [
    "#### Create a new dataframe that has only the rows where income != -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb76a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe that has only the rows where income != -1\n",
    "df_hasincome = df[df.income != -1]\n",
    "print(df_hasincome.info())\n",
    "print(len(df_hasincome.age))\n",
    "print(df_hasincome.columns)\n",
    "print(df_hasincome.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b506e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells us there are null values somewhere in the df\n",
    "print(\"(df_hasincome.isnull().values.any())\")\n",
    "print(df_hasincome.isnull().values.any())\n",
    "\n",
    "# This prints out the sum of null values by column \n",
    "print(\"(df_hasincome.isnull().sum())\")\n",
    "print(df_hasincome.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a8d18e",
   "metadata": {},
   "source": [
    "Fun thing: Noticed the 3 people for whom a response to height was missing were not included in this data set <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2848aff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_hasincome.income.nunique())\n",
    "print(df_hasincome.income.unique())\n",
    "# 0ne-way freq table of income\n",
    "print(pd.crosstab(index=df_hasincome.income, columns='count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7883b72",
   "metadata": {},
   "source": [
    "I wonder if the income distribution of respondents is patterned similarly to that of the San Francisco income distribution at the time of collection. I'm putting this here because it is a thought I had at this point in the analysis, and it's my analysis. I would need to confirm the year of the data collection. I think Kaggle said 2015, but the data owner GitHub said \"2010s\". I will have to search data owner GitHub for a year later. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05856595",
   "metadata": {},
   "source": [
    "Maybe it is because it is late and I am tired, but now I'm wondering if since the income data is already binned, is it considered categorical? Would a hisogram be appropriate? This is definitely me being tired and a sign that it is time for bed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989cb422",
   "metadata": {},
   "source": [
    "### Notes to self for later\n",
    "\n",
    "Best rounding practices? <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e151a6",
   "metadata": {},
   "source": [
    "### Possible things I can do with data:\n",
    "\n",
    "make sure you're using the words \"value\" and \"variable\" right <br>\n",
    "\n",
    "deal with missing values <br>\n",
    "\n",
    "1. split the sex column into two columns: male and female OR I change the sex column to be male = 1 femaile = 0\n",
    "2. maybe take advantage of age being a number to do some quantitative analyses \n",
    "3. make demographics tables (including treating age as a categorical variable)\n",
    "4. I can potentially do quantitative analyses with income \n",
    "5. This is your reminder to ADD HYPERLINKS TO read_me.md LATER <br>\n",
    "to be continued"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b083d0b3",
   "metadata": {},
   "source": [
    "### On trying to figure out how to have print make prettier tables\n",
    "###### 10/1/21 2:45pm ish\n",
    "\n",
    "I was looking into indexing and found this towardsdatacience article.\n",
    "\n",
    "https://towardsdatascience.com/pandas-index-explained-b131beaf6f7b\n",
    "\n",
    "While reading the towardsdatacience article, I noticed they were getting these pretty table output when they called dataframe.head() in their notebook. So now I'm learning how to get the pretty table output, and I think it might have something to do with IPython. So I'm gonna work on understanding IPython now. And figuring out how I can get the nice pretty table display. \n",
    "\n",
    "https://ipython.readthedocs.io/en/stable/interactive/tutorial.html\n",
    "\n",
    "This was suggested by Codecademy for \"pretty display.....#Displays all variables and statements on its own line\" and I have no idea how it is helping me. \n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" <br>\n",
    "https://www.codecademy.com/paths/data-analyst/tracks/dacp-python-fundamentals/modules/dscp-getting-started-off-platform/articles/getting-more-out-of-jupyter-notebook\n",
    "\n",
    "###### 10/1/21 4:06pm \n",
    "I give up on this effort to get the pretty df.head() table display. I think it might have something to do wtih widgets. Every article I find when I search terms related to \"pretty table display jupyter notebook\" seem to be not related. Like they already have the pretty tables and are talking about resizing them or something. \n",
    "It might also have something to do with display\n",
    "\n",
    "https://ipywidgets.readthedocs.io/en/latest/ <br>\n",
    "https://stackoverflow.com/questions/26873127/show-dataframe-as-table-in-ipython-notebook <br>\n",
    "https://songhuiming.github.io/pages/2017/04/02/jupyter-and-pandas-display/ <br>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2c5a610fc58fc270e82fdda9722a5bb26482036711b86ea7e805490427c7c4f9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
